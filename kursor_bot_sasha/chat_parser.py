"""
–ü–∞—Ä—Å–µ—Ä —ç–∫—Å–ø–æ—Ä—Ç–∞ —á–∞—Ç–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤
–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∏—Ö –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –±–æ—Ç–æ–≤
"""
import json
import re
import random
from typing import List, Dict, Any
from datetime import datetime

class ChatParser:
    def __init__(self, chat_export_path: str):
        self.chat_export_path = chat_export_path
        self.natural_responses = {
            'general': [],
            'questions': [],
            'statements': [],
            'emotional': [],
            'conversation_starters': []
        }
        
    def parse_chat_export(self) -> Dict[str, List[str]]:
        """–ü–∞—Ä—Å–∏—Ç —ç–∫—Å–ø–æ—Ä—Ç —á–∞—Ç–∞ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∏–∞–ª–æ–≥–∏"""
        try:
            print(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º —ç–∫—Å–ø–æ—Ä—Ç —á–∞—Ç–∞: {self.chat_export_path}")
            
            with open(self.chat_export_path, 'r', encoding='utf-8') as f:
                chat_data = json.load(f)
            
            messages = chat_data.get('messages', [])
            print(f"üìä –ù–∞–π–¥–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {len(messages)}")
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
            self._analyze_messages(messages)
            
            return self.natural_responses
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —á–∞—Ç–∞: {e}")
            return {}
    
    def _analyze_messages(self, messages: List[Dict[str, Any]]):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∏—Ö"""
        processed_count = 0
        
        for msg in messages:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
            if msg.get('type') != 'message':
                continue
                
            text = self._extract_text(msg)
            if not text or len(text) < 5:
                continue
            
            # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
            clean_text = self._clean_text(text)
            if not clean_text:
                continue
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
            self._classify_message(clean_text)
            processed_count += 1
            
            if processed_count % 1000 == 0:
                print(f"üìù –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {processed_count}")
        
        print(f"‚úÖ –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_count}")
        self._print_statistics()
    
    def _extract_text(self, msg: Dict[str, Any]) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è"""
        text = msg.get('text', '')
        
        # –ï—Å–ª–∏ text - —ç—Ç–æ —Å–ø–∏—Å–æ–∫ (—Å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º)
        if isinstance(text, list):
            text_parts = []
            for item in text:
                if isinstance(item, str):
                    text_parts.append(item)
                elif isinstance(item, dict) and 'text' in item:
                    text_parts.append(item['text'])
            text = ''.join(text_parts)
        
        return str(text).strip()
    
    def _clean_text(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        # –£–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏
        text = re.sub(r'http[s]?://\S+', '', text)
        
        # –£–±–∏—Ä–∞–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        text = re.sub(r'@\w+', '', text)
        
        # –£–±–∏—Ä–∞–µ–º —Ö–µ—à—Ç–µ–≥–∏
        text = re.sub(r'#\w+', '', text)
        
        # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        
        # –£–±–∏—Ä–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è (–±–æ–ª–µ–µ 200 —Å–∏–º–≤–æ–ª–æ–≤)
        if len(text) > 200:
            return ""
        
        # –£–±–∏—Ä–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –∏–∑ —ç–º–æ–¥–∑–∏
        if re.match(r'^[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\s]+$', text):
            return ""
        
        return text.strip()
    
    def _classify_message(self, text: str):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ —Ç–∏–ø—É"""
        text_lower = text.lower()
        
        # –í–æ–ø—Ä–æ—Å—ã
        question_patterns = [
            r'\?', r'–∫–∞–∫\s+', r'—á—Ç–æ\s+', r'–≥–¥–µ\s+', r'–∫–æ–≥–¥–∞\s+', 
            r'–ø–æ—á–µ–º—É\s+', r'–∑–∞—á–µ–º\s+', r'–∫—Ç–æ\s+'
        ]
        
        if any(re.search(pattern, text_lower) for pattern in question_patterns):
            self.natural_responses['questions'].append(text)
            return
        
        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        emotional_words = [
            '–∫—Ä—É—Ç–æ', '–æ—Ç–ª–∏—á–Ω–æ', '—Å—É–ø–µ—Ä', '–∫–ª–∞—Å—Å', '–æ—Ñ–∏–≥–µ—Ç—å', '–≤–∞—É', 
            '–±–ª–∏–Ω', '—á–µ—Ä—Ç', '–∫–∞–ø–µ—Ü', '–∂–µ—Å—Ç—å', '–æ–±–æ–∂–∞—é', '–Ω–µ–Ω–∞–≤–∏–∂—É',
            '–ø—Ä–∏–∫–æ–ª—å–Ω–æ', '–∑–¥–æ—Ä–æ–≤–æ', '–±–æ–º–±–∞', '–æ–≥–æ–Ω—å'
        ]
        
        if any(word in text_lower for word in emotional_words):
            self.natural_responses['emotional'].append(text)
            return
        
        # –ù–∞—á–∞–ª–∞ —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤
        starter_patterns = [
            r'^–ø—Ä–∏–≤–µ—Ç', r'^–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π', r'^–¥–æ–±—Ä', r'^—Ä–µ–±—è—Ç', 
            r'^–Ω–∞—Ä–æ–¥', r'^—Å–ª—É—à–∞–π', r'^–∫—Å—Ç–∞—Ç–∏'
        ]
        
        if any(re.search(pattern, text_lower) for pattern in starter_patterns):
            self.natural_responses['conversation_starters'].append(text)
            return
        
        # –û–±—ã—á–Ω—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
        if len(text.split()) > 2 and not text.endswith('?'):
            self.natural_responses['statements'].append(text)
        else:
            self.natural_responses['general'].append(text)
    
    def _print_statistics(self):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏—è–º"""
        print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π:")
        for category, messages in self.natural_responses.items():
            print(f"  {category}: {len(messages)}")
    
    def get_random_response(self, category: str = 'general') -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        responses = self.natural_responses.get(category, [])
        return random.choice(responses) if responses else ""
    
    def save_to_files(self, output_dir: str = "kursor bot sasha/"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–∞–π–ª—ã"""
        try:
            import os
            
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–π —Ä–µ—á–∏
            natural_speech_dir = os.path.join(output_dir, "natural_speech")
            os.makedirs(natural_speech_dir, exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª
            for category, messages in self.natural_responses.items():
                if messages:
                    filename = os.path.join(natural_speech_dir, f"{category}.json")
                    with open(filename, 'w', encoding='utf-8') as f:
                        json.dump({
                            'category': category,
                            'count': len(messages),
                            'messages': messages
                        }, f, ensure_ascii=False, indent=2)
                    
                    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(messages)} —Å–æ–æ–±—â–µ–Ω–∏–π –≤ {filename}")
            
            print(f"‚úÖ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {natural_speech_dir}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {e}")

if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    parser = ChatParser("../ChatExport_2025-08-05/result.json")
    natural_data = parser.parse_chat_export()
    parser.save_to_files()